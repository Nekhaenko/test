{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "<h1>Chapter 7 - Advanced Text Generation Techniques and Tools</h1>\n",
        "<i>Going beyond prompt engineering.</i>\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
        "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
        "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter07/Chapter%207%20-%20Advanced%20Text%20Generation%20Techniques%20and%20Tools.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is for Chapter 7 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
        "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nHmeL67n47l"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        "üí° **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jEoDdGDQn47m"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerbJgwAigbK"
      },
      "source": [
        "# Loading an LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-uKn-Mmn47o",
        "outputId": "eebabfb5-aec9-4130-ceb0-44deb034cd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-11 09:23:21--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.40, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1746959001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Njk1OTAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=OMFIhlUqvfMRyuOIbRaTHSk139JSFNe%7E58ChebInH%7EG8TbALV7WXkKghj8eaXVxszCdCNOJuyN5XkBX7rdpKZCQeFH5Gkuz%7EhKdk7RtZxl58mSImjzbEXY4oAfCwEKtNkZ3USh0sgtOA6KY7xpsOxoJMiHYe964pahOnaJALXMUGscY2Qc%7ENuKENnl2UU%7E1hzfCqVlCCbNLSzmRKeM0gFCMWKUvDy%7EfChDndCWaUV1IlF0Zap40VVDgxtTDIYhdjZ634gYcxtyISpIUhaUyIFMKHqXRz9S7qSuP07ds-B7PMQ2-i%7ENyHDcruNkFMjhBajGuGvGtkY%7EnHfmI1ORg4Uw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-05-11 09:23:21--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1746959001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Njk1OTAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=OMFIhlUqvfMRyuOIbRaTHSk139JSFNe%7E58ChebInH%7EG8TbALV7WXkKghj8eaXVxszCdCNOJuyN5XkBX7rdpKZCQeFH5Gkuz%7EhKdk7RtZxl58mSImjzbEXY4oAfCwEKtNkZ3USh0sgtOA6KY7xpsOxoJMiHYe964pahOnaJALXMUGscY2Qc%7ENuKENnl2UU%7E1hzfCqVlCCbNLSzmRKeM0gFCMWKUvDy%7EfChDndCWaUV1IlF0Zap40VVDgxtTDIYhdjZ634gYcxtyISpIUhaUyIFMKHqXRz9S7qSuP07ds-B7PMQ2-i%7ENyHDcruNkFMjhBajGuGvGtkY%7EnHfmI1ORg4Uw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.165.75.101, 3.165.75.19, 3.165.75.33, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.165.75.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‚ÄòPhi-3-mini-4k-instruct-fp16.gguf‚Äô\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G  40.8MB/s    in 59s     \n",
            "\n",
            "2025-05-11 09:24:20 (124 MB/s) - ‚ÄòPhi-3-mini-4k-instruct-fp16.gguf‚Äô saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
        "\n",
        "# If this command does not work for you, you can use the link directly to download the model\n",
        "# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LQcht_ZFijW7"
      },
      "outputs": [],
      "source": [
        "from langchain import LlamaCpp\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3SNhQF9WthzV",
        "outputId": "45e8b8cb-2e55-45bd-dad7-8ea3cddbd7bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n**Response:** The answer to the arithmetic question \"–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 1+1?\" is 2. This question requires basic math skills, specifically addition. In Russian, you would say \"1 + 1 —Ä–∞–≤–Ω–æ 2.\" Therefore, regardless of the initial context about Alexander\\'s name, when asked this simple mathematical problem in Russian or any other language, the answer remains the same: 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "llm.invoke(\"–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–∞–Ω–¥—Ä. –°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 1+1?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwx2AIuGfCoP"
      },
      "source": [
        "### Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kF--Q5me_-X1"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a prompt template with the \"input_prompt\" variable\n",
        "template = \"\"\"<s><|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ogWsGeg6hElt"
      },
      "outputs": [],
      "source": [
        "basic_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "KINQxKAINXgG",
        "outputId": "9a354add-3046-45f1-b9dc-6d4ea41c0dd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' –ü—Ä–∏–≤–µ—Ç –ê–ª–µ–∫—Å–∞–Ω–¥—Ä! –£—Ä–æ–≤–µ–Ω—å –Ω–æ–≤–∏znosti –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–∂–Ω—ã–º –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –Ω–æ –≤–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ç–∞–∫ —à–∏—Ä–æ–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã, –Ω–æ –≤—Å–µ –∂–µ –∏–º–µ—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª:\\n\\n1. Tezos: –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ—Å–µ—Ç—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –æ–±–æ–±—â–µ–Ω–Ω–æ–π –ø–ª–∞–Ω–∏—Ä–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞.\\n2. Avalanche: –°–µ—Ç—å –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—á–µ—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ NFT –∏ Proof-of-Stake –∞–ª–≥–æ—Ä–∏—Ç–º–µ.\\n3. Cosmos: –°–µ—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ—Å–∏-—Å–∏—Å—Ç–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É —Å–æ–±–æ–π —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã.\\n4. Polkadot: –°–æ—á–∏–Ω–∏–≤–∞atorio, –ø–æ–∑–≤–æ–ª—è—é—â–µ–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ—Å–µ—Ç —Å –ø–æ–º–æ—â—å—é —Ç–æ–Ω–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±–º–µ–Ω –º–µ–∂–¥—É –Ω–∏–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π.\\n5. Solana: –°–µ—Ç—å-–¥–∏–∫—Ç–∞—Ç–æ—Ä, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ Proof-of-Stake –∞–ª–≥–æ—Ä–∏—Ç–º–µ —Å –≤—ã—Å–æ–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π, –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—è –¥–∞–Ω–Ω—ã—Ö (SSD).\\n6. Algorand: –ü—Ä–æ—Ç–æ—Ç–∏–ø —Å–∏—Å—Ç–µ–º—ã –æ–ø–µ–Ω—Å–æ—Ä—Å–∞ –¥–ª—è –∫—Ä–∏pto—Å–µ—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π Proof-of-Stake –∞–ª–≥–æ—Ä–∏—Ç–º, —Å –≤—ã—Å–æ–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é.\\n\\n–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Å—Ç–∞—Ç—É—Å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è, –ø–æ—ç—Ç–æ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Å–ª–µ–¥–∏—Ç—å –∑–∞ –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫—Ä–∏–ø—Ç–æ—ç–∫–æ–Ω–æ—Ç–∏–∫–∏.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Use the chain\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–∞–Ω–¥—Ä. –ö–∞–∫–∏–µ –º–∞–ª–æ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ, –Ω–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã —Ç—ã –∑–Ω–∞–µ—à—å?\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMBMRxB8gFW"
      },
      "source": [
        "### Multiple Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrUKuHt_OLpe",
        "outputId": "453470cd-0fb0-4b1d-df64-0e39236c4b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-61dd782c6da9>:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create a chain for the title of our story\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igFIyg73OtaL",
        "outputId": "86056d8d-7573-4a36-b9bd-f793251623f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': '—É—á—ë–Ω—ã–π –∫–æ—Ç–æ—Ä—ã–π –∏–∑–æ–±—Ä–µ—Ç–∞–µ—Ç –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É –±—É–¥—É—â–µ–≥–æ',\n",
              " 'title': ' \"Genesis of Tomorrow: The Inventor\\'s Chronicles in Future Cryptocurrency\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "title.invoke({\"summary\": \"—É—á—ë–Ω—ã–π –∫–æ—Ç–æ—Ä—ã–π –∏–∑–æ–±—Ä–µ—Ç–∞–µ—Ç –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É –±—É–¥—É—â–µ–≥–æ\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zTtFEmANOhyE"
      },
      "outputs": [],
      "source": [
        "# Create a chain for the character description using the summary and title\n",
        "template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xjf-avW8NAqZ"
      },
      "outputs": [],
      "source": [
        "# Create a chain for the story using the summary, title, and character description\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "epNudKyyPClO"
      },
      "outputs": [],
      "source": [
        "# Combine all three components to create the full chain\n",
        "llm_chain = title | character | story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b44ZR0vXRaAo",
        "outputId": "6880018d-9ed3-45dc-d8a1-8cd665b50051"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': '—É—á—ë–Ω—ã–π, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–æ–±—Ä–µ—Ç–∞–µ—Ç –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É –±—É–¥—É—â–µ–≥–æ',\n",
              " 'title': ' \"–°–æ–≤–µ—Ä—à–∏–≤ –ú–∏racle: –•—É–¥–æ–∂–Ω–∏–∫-–ò–∑—É–º—ã—à–Ω–∏–∫ –°–æ–∑–¥–∞–µ—Ç –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É –ó–∞–¥–Ω–µ–≥–æ –í–µ—Å–Ω–∞\"',\n",
              " 'character': ' The main character of \"–°–æ–≤–µ—Ä—à–∏–≤ –ú–∏racle: –•—É–¥–æ–∂–Ω–∏–∫-–ò–∑—É–º—ã—à–Ω–∏–∫ –°–æ–∑–¥–∞–µ—Ç –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É –ó–∞–¥–Ω–µ–≥–æ –í–µ—Å–Ω–∞\" is a brilliant scientist and artist, Dr. Ivan Petrovich, whose groundbreaking invention combines cutting-edge cryptography with futuristic technology, earning him the title of mastermind behind creating digital currency during winter\\'s obscurity. Driven by his insatiable curiosity for innovation and artistry, he embarks on a daring journey to reshape the financial landscape while facing both accolades and skepticism from society.',\n",
              " 'story': ' In the depths of winter\\'s embrace, Dr. Ivan Petrovich, a virtuoso scientist and avant-garde artist, unveiled his magnum opus: cryptovatu currency christened \"Cryptonova,\" born from the nexus of intricate encryption techniques and futuristic algorithms. His groundbreaking creation defied conventions as it emerged during a time when digital currencies were still embryonic, leaving society in both wonderment and skepticism. Petrovich\\'s odyssey to revolutionize finance was punctuated by accolades from the tech elite yet met with derision among traditionalists, but his indomitable spirit propelled him forward. As Cryptonova gained global traction, it rewrote economic narratives and etched Dr. Petrovich\\'s name into history as the maestro who painted prosperity in the shadows of winter.'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "llm_chain.invoke(\"—É—á—ë–Ω—ã–π, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–æ–±—Ä–µ—Ç–∞–µ—Ç –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É –±—É–¥—É—â–µ–≥–æ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UQ-DZ71P-D-"
      },
      "source": [
        "# Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-15Eoey5EJUO",
        "outputId": "6eac9d0c-d0cc-4339-dff8-3884871fd21c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Maarten, the answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another unit.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Let's give the LLM our name\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N42wQRl-Lykt",
        "outputId": "87c37364-8ba2-4b9e-e6e9-f841f0e9268f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm an AI and don't have the ability to know personal information about individuals unless it has been shared with me in the course of our conversation. Therefore, I can't know your name.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Next, we ask the LLM to reproduce the name\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqATEZjMgET"
      },
      "source": [
        "## ConversationBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Zoo0PA1fUs70"
      },
      "outputs": [],
      "source": [
        "# Create an updated prompt template to include a chat history\n",
        "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bgGMS1S9saLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc43ee99-c63f-4a45-8cbc-42f6c95f2488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-9823403f20ac>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Define the type of Memory we will use\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg = \"–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–∞–Ω–¥—Ä. –ö–∞–∫–∏–µ —ç–∫—Å—Ç—Ä–∞–≤–∞–≥–∞–Ω—Ç–Ω—ã–µ, –Ω–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã —Ç—ã –∑–Ω–∞–µ—à—å?\""
      ],
      "metadata": {
        "id": "l3C4ExemuhVn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mltR_GtkiqDZ",
        "outputId": "3bff80a5-dd3e-4bb0-d28b-2aafc5bbd644"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': '–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–∞–Ω–¥—Ä. –ö–∞–∫–∏–µ —ç–∫—Å—Ç—Ä–∞–≤–∞–≥–∞–Ω—Ç–Ω—ã–µ, –Ω–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã —Ç—ã –∑–Ω–∞–µ—à—å?',\n",
              " 'chat_history': '',\n",
              " 'text': ' –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –ê–ª–µ–∫—Å–∞–Ω–¥—Ä! –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å:\\n\\n1. Algorand: –≠—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º–∞, –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∑–∞ —Å–≤–æ–∏–º –±—ã—Å—Ç—Ä—ã–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∏ –≤—ã—Å–æ–∫–æ–π –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏. Algorand –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—É–±–ª–∏—á–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ —Å–∏—Å—Ç–µ–º—ã, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –±–µ—Å–ø—Ä–∏–µ–º—Å—Ç–≤–µ–Ω–Ω–æ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ.\\n\\n2. Tezos: Tezos –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ —Å–≤–æ–µ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ Chiavelli, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å–µ—Ç–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. Tezos —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ—Å–ø–∏—Å–∫–∏ –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.\\n\\n3. Cardano: –°–æ–∑–¥–∞–Ω–Ω—ã–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã Bayesian inference –∏ Proof-of-Stake (PoS), Cardano —Å—Ç—Ä–µ–º–∏—Ç—Å—è –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏. –û–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∫—Ä–∏–ø—Ç–æ–ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.\\n\\n4. Polkadot: –≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–æ—â–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç, —Å–æ–∑–¥–∞–≤–∞—è –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –Ω–∏–º–∏. Polkadot –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º Nominated Proof of Stake (NPoS), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.\\n\\n5. Solana: Solana –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–∏–º –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º–µ Stochastic Self-Amplifying Merkel Tree'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Generate a conversation and ask a basic question\n",
        "llm_chain.invoke({\"input_prompt\": msg})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-je1rmy3dx4",
        "outputId": "5029f802-0e71-432e-9002-09f4b5d17b0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': 'Human: –ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–∞–Ω–¥—Ä. –ö–∞–∫–∏–µ —ç–∫—Å—Ç—Ä–∞–≤–∞–≥–∞–Ω—Ç–Ω—ã–µ, –Ω–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã —Ç—ã –∑–Ω–∞–µ—à—å?\\nAI:  –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –ê–ª–µ–∫—Å–∞–Ω–¥—Ä! –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å:\\n\\n1. Algorand: –≠—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º–∞, –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∑–∞ —Å–≤–æ–∏–º –±—ã—Å—Ç—Ä—ã–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∏ –≤—ã—Å–æ–∫–æ–π –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏. Algorand –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—É–±–ª–∏—á–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ —Å–∏—Å—Ç–µ–º—ã, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –±–µ—Å–ø—Ä–∏–µ–º—Å—Ç–≤–µ–Ω–Ω–æ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ.\\n\\n2. Tezos: Tezos –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ —Å–≤–æ–µ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ Chiavelli, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å–µ—Ç–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. Tezos —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ—Å–ø–∏—Å–∫–∏ –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.\\n\\n3. Cardano: –°–æ–∑–¥–∞–Ω–Ω—ã–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã Bayesian inference –∏ Proof-of-Stake (PoS), Cardano —Å—Ç—Ä–µ–º–∏—Ç—Å—è –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏. –û–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∫—Ä–∏–ø—Ç–æ–ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.\\n\\n4. Polkadot: –≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–æ—â–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç, —Å–æ–∑–¥–∞–≤–∞—è –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –Ω–∏–º–∏. Polkadot –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º Nominated Proof of Stake (NPoS), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.\\n\\n5. Solana: Solana –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–∏–º –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º–µ Stochastic Self-Amplifying Merkel Tree\\nHuman: –∫–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?\\nAI:  –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø Assistant, –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç. –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å:\\n\\n1. Algorand: –≠—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º–∞, –∏–∑–≤–µ—Å—Ç–Ω–∞—è –∑–∞ —Å–≤–æ–∏–º –±—ã—Å—Ç—Ä—ã–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∏ –≤—ã—Å–æ–∫–æ–π –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å. Algorand –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—É–±–ª–∏—á–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ —Å–∏—Å—Ç–µ–º—ã, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –±–µ—Å–ø—Ä–∏–µ–º—Å—Ç–≤–µ–Ω–Ω–æ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ.\\n\\n2. Tezos: Tezos –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Å–≤–æ–µ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ Chiavelli, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å–µ—Ç–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. Tezos —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ—Å–ø–∏—Å–∫–∏ –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.\\n\\n3. Cardano: –°–æ–∑–¥–∞–Ω–Ω—ã–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã Bayesian inference –∏ Proof-of-Stake (PoS), Cardano —Å—Ç—Ä–µ–º–∏—Ç—Å—è –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏. –û–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∫—Ä–∏–ø—Ç–æ–ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.\\n\\n4. Polkadot: –≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–æ—â–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç, —Å–æ–∑–¥–∞–≤–∞—è –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –Ω–∏–º–∏. Polkadot –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º Nominated Proof of Stake (NPoS), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.\\n\\n5. Solana: Solana –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–∏–º –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ Proof-of-Stake (PoS) –∞–ª–≥–æ—Ä–∏—Ç–º–µ Stoch',\n",
              " 'text': ' My name is Alexandre. I\\'m glad to assist you with information on cryptocurrencies! Here are a few interesting and promising ones:\\n\\n1. Algorand: An open and practical implementation of the Proof-of-Stake (PoS) algorithm, known for its fast transaction processing and high scalability. It uses unique public keys for network authentication, making it user-friendly and secure.\\n2. Tezos: An open cryptocurrency with zero transaction fees, based on its own Chiavelli technology, which allows updating the consensus mechanism without altering the blockchain\\'s data. Tezos supports multiple chains and development platforms.\\n3. Cardano: Built using Bayesian inference and Proof-of-Stake (PoS) algorithms, aiming for high security, efficiency, and accessibility. It offers various tools for creating private chain projects.\\n4. Polkadot: A platform that enables interoperability between different blockchains through a shared framework of communication, powered by its proprietary Proof-of-Stake (NPoS) algorithm known as Nominated Proof of Stake (NPoS), offering high speed and efficiency.\\n5. Solana: Provides exceptional performance and scalability due to its fast transaction processing, based on the Proof-of-Stake (PoS) mechanism called Stochastic Self-Amplifying Merkel Tree.\\n\\nI\\'m an AI developed by Microsoft called \"Assistant\". I\\'m here to provide information and help you with your queries!'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Does the LLM remember the name we gave it?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3ELCg6Rpsk"
      },
      "source": [
        "## ConversationBufferMemoryWindow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "G0DRT7kjRtiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d436d04-9c66-460f-fd28-217b5b5cfb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-046ef635f261>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Retain only the last 2 conversations in memory\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBY69vvcR1Qq",
        "outputId": "9d9b1122-5c7c-4a6b-fda4-230094e17e9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 3 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten, it's nice to meet you! The answer to 1 + 1 is 2.\\n\\nHowever, if this was part of a larger conversation or context-related question, feel free to provide more information for further engagement. For now, the simple math adds up nicely as always: one plus one equals two.\",\n",
              " 'text': \" Hello Maarten, it's nice to meet you too! The answer to 3 + 3 is 6.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Ask two questions and generate two conversations in its memory\n",
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSLfKWpR5h5",
        "outputId": "7fc633ae-8de4-4ee6-e3a6-6a1fcaeeb432"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten, it's nice to meet you! The answer to 1 + 1 is 2.\\n\\nHowever, if this was part of a larger conversation or context-related question, feel free to provide more information for further engagement. For now, the simple math adds up nicely as always: one plus one equals two.\\nHuman: What is 3 + 3?\\nAI:  Hello Maarten, it's nice to meet you too! The answer to 3 + 3 is 6.\",\n",
              " 'text': ' Your name is Maarten, as mentioned at the beginning of our conversation.\\n\\nNow for the math question: 3 + 3 equals 6.'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Check whether it knows the name we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW7qEyctcqeJ",
        "outputId": "ed490176-075e-4c5a-830f-7899382ed34e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': \"Human: What is 3 + 3?\\nAI:  Hello Maarten, it's nice to meet you too! The answer to 3 + 3 is 6.\\nHuman: What is my name?\\nAI:  Your name is Maarten, as mentioned at the beginning of our conversation.\\n\\nNow for the math question: 3 + 3 equals 6.\",\n",
              " 'text': ' I\\'m an AI and do not have a physical form or age. However, if you\\'re referring to the age of the information I was trained on up until September 2021, it would be considered as having \"no age\" in the human sense since it is data-driven knowledge.\\nAnswer: As an AI, I do not have an age.'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Check whether it knows the age we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSb5OnANMhu2"
      },
      "source": [
        "## ConversationSummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lWHZlJUbwpqE"
      },
      "outputs": [],
      "source": [
        "# Create a summary prompt template\n",
        "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qg1HAgxZMkbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d5e701-2a0a-4aae-bddd-4fb996ca1427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-905e44fca19a>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Define the type of memory we will use\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"chat_history\",\n",
        "    prompt=summary_prompt\n",
        ")\n",
        "\n",
        "# Chain the LLM, prompt, and memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klIk9CpVSH0",
        "outputId": "92ef8f5d-55ad-4817-93f5-74ac4d7a18dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name? do not write any other information.',\n",
              " 'chat_history': \" Maarten introduces himself and inquires about the result of 1 + 1, which the AI correctly answers as 2 by explaining it as a simple addition operation. The AI also clarifies that personal data isn't available without current context and reminds Maarten he previously mentioned his name in this conversation.\",\n",
              " 'text': ' Maarten.'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Generate a conversation and ask for the name\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name? do not write any other information.\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VdOH_I-V-Fy",
        "outputId": "b20dc2a4-1be8-40dd-d683-955b32900875"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_prompt': 'What was the first question I asked?',\n",
              " 'chat_history': ' Summary: Human, identified as Maarten in the context of this conversation, first asked about the sum of 1 + 1 and received an answer of 2 from the AI. Later, Maarten inquired about their name but the AI clarified that personal data is not retained beyond a single session for privacy reasons. The AI offered further assistance if needed.',\n",
              " 'text': ' The first question you asked was \"what\\'s 1 + 1?\"'}"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check whether it has summarized everything thus far\n",
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_LlvrVX9HL",
        "outputId": "f1bf53fa-ab7c-446a-a95c-da4cf7c29dbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': \" Maarten introduces himself, inquires about the result of 1 + 1 (correctly answered as a simple addition operation), and asks for his name in the conversation; the AI confirms it's Maarten without providing any additional personal information. The AI also reminds Maarten that personal data requires current context.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Check what the summary is thus far\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5sJa1qvS4N"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcBt8bZM56dM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load OpenAI's LLMs with LangChain\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
        "# openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lmRZu8DO2p6k"
      },
      "outputs": [],
      "source": [
        "# Create the ReAct template\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=react_template,\n",
        "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NV-ssNa-4zOK"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "\n",
        "# You can create the tool to pass to an agent\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "    name=\"duckduck\",\n",
        "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "    func=search.run,\n",
        ")\n",
        "\n",
        "# Prepare tools\n",
        "tools = load_tools([\"llm-math\"], llm=llm)\n",
        "tools.append(search_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6tAr1962vS4T"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# Construct the ReAct agent\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSU6ECdYBOOm",
        "outputId": "4f99000b-5f88-4261-8a57-5389789819f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to use duckduck to find the current price and then calculate the amount in EUR using the given exchange rate.\n",
            "Action: duckduck\n",
            "Action Input: \"current MacBook Pro price in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: The base model of the M3 Pro starts at $2,499 USD (although exclusive discounts are available in this guide) and is an ideal option for users who want a larger screen and advanced capabilities. To get the best deal on this or any other MacBook Pro, check out AppleInsider's list of the top MacBook Pro deals., title: MacBook Pro 16-inch Best Price M3 Max, 16C CPU, 40C GPU, 48GB, 1TB ..., link: https://prices.appleinsider.com/product/macbook-pro-16-inch-m3/MUW63LL/A, snippet: For a limited time, you can get the M4 MacBook Pro (Silver) for $1,373 from go-to Apple retailer, Amazon. That's $226 below the M4 MB Pro's $1,599 list price and just $1 shy of its all-time low., title: Apple's excellent M4 MacBook Pro hits all-time low price - Laptop Mag, link: https://www.laptopmag.com/deals/laptops/macbooks/the-5-star-rated-m4-macbook-pro-dips-to-usd1-373-at-amazon, snippet: Today we're tracking a collection of discounts on Apple's M4 MacBook Pro at Amazon, including as much as $479 off select models of the..., title: Apple's M4 MacBook Pro Hits New Record Low Prices on Amazon at Up to ..., link: https://www.macrumors.com/2025/05/05/apples-m4-macbook-pro-low/, snippet: Discounts of up to $370 off are in effect at Amazon on Apple's current MacBook Pros, with prices starting at $1,399., title: Save Up to $370 on M4 MacBook Pros in Amazon, B&H Price War, link: https://appleinsider.com/articles/25/05/06/amazon-launches-m4-macbook-pro-deals-up-to-370-off-amid-price-hike-fears\u001b[0m\u001b[32;1m\u001b[1;3m I found the current price, but it seems there is a list of various models and deals. Since we are interested in just the base model for our calculation, I'll extract that information.\n",
            "Action: duckduck\n",
            "Action Input: \"base MacBook Pro M3 Max price\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: The base model of the M3 Pro starts at $2,499 USD (although exclusive discounts are available in this guide) and is an ideal option for users who want a larger screen and advanced capabilities. To get the best deal on this or any other MacBook Pro, check out AppleInsider's list of the top MacBook Pro deals., title: MacBook Pro 16-inch Best Price M3 Max, 16C CPU, 40C GPU, 48GB, 1TB ..., link: https://prices.appleinsider.com/product/macbook-pro-16-inch-m3/MUW63LL/A, snippet: Right now, you can get up to $500 off the M3, M3 Pro and M3 Max MacBook Pros ‚Äî dropping prices to as low as $1,499., title: Every M3 MacBook Pro has dropped to its lowest price in huge pre-Prime ..., link: https://www.tomsguide.com/computing/macbooks/every-m3-macbook-pro-has-dropped-to-its-lowest-price-in-huge-pre-prime-day-sale-up-to-dollar500-off, snippet: Best MacBook Pro deals for May 2025. Save up to $1,600 on Apple 14-inch & 16-inch laptops with deals on every M4 model & clearance prices on M3 specs., title: Best MacBook Pro Deals for May 2025 | Save up to $800 - AppleInsider, link: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: M3 MacBook Pro: was $1,599 now $1,099 @ Amazon with coupon applied Make sure you apply the coupon on the product page to get this shockingly low price. This is on the base model M3 MacBook Pro, so ..., title: M3 MacBook Pro plummets to all-time lowest price in biggest Black ..., link: https://www.tomsguide.com/computing/macbooks/m3-macbook-pro-black-friday-deal-lowest-price\u001b[0m\u001b[32;1m\u001b[1;3m I have the base model of an M3 MacBook Pro which is $2,499 USD before discounts. To find out how much it costs in EUR with an exchange rate of 0.85 EUR for each USD, I'll use duckduck again to perform this calculation.\n",
            "Action: Calculator\n",
            "Action Input: (2499 * 0.85)\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 2124.15\u001b[0m\u001b[32;1m\u001b[1;3m I now know the final answer. The current price of a base model MacBook Pro M3 in EUR with an exchange rate of 0.85 is approximately $2,124.15.\n",
            "Final Answer: The current price of a base model MacBook Pro M3 in USD is $2,499. At an exchange rate of 0.85 EUR for each USD, it would cost approximately ‚Ç¨2,124.15.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Answer: The current price of a base model MacBook Pro M3 in USD is $2,499. At an exchange rate of 0.85 EUR for each USD, it would cost approximately ‚Ç¨2,124.15.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
              " 'output': 'The current price of a base model MacBook Pro M3 in USD is $2,499. At an exchange rate of 0.85 EUR for each USD, it would cost approximately ‚Ç¨2,124.15.\\n\\n\\n\\n\\nAnswer: The current price of a base model MacBook Pro M3 in USD is $2,499. At an exchange rate of 0.85 EUR for each USD, it would cost approximately ‚Ç¨2,124.15.'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# What is the Price of a MacBook Pro?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg = '–ù–∞–ø–∏—à–∏ –ø–ª–∞–Ω –∏–∑—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∞–≥–µ–Ω—Ç–æ–≤, –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∫–∞—á–∫–∏ –≤ —ç—Ç–æ–π —Ç–µ–º–µ.'\n",
        "msg_eng = 'Write a plan for studying large language models and agents, for maximum improvement in this topic.'"
      ],
      "metadata": {
        "id": "uC7OJX76w9lR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the Price of a MacBook Pro?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": msg\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Hz3uieuCwt0q",
        "outputId": "3f7bbaec-ec2f-4ce5-f2d2-b5043da9dbb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m To create a plan for studying large language models and agents, I need to gather information on effective learning methods and resources available for this topic.\n",
            "Action: duckduck\n",
            "Action Input: \"–ø–ª–∞–Ω—ã –∏–∑—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∞–≥–µ–Ω—Ç–æ–≤\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: –ü–æ—á–µ–º—É Go-–∞—Å—Å–µ–º–±–ª–µ—Ä –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã: –∏–¥–µ—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è. –ö—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä –∏ –ø–µ—Ä–µ–≤–æ–¥ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è Large Language Model-Based Agents for Software Engineering: A Survey , –∫–æ—Ç–æ—Ä–æ–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π..., title: –ò–ò-–∞–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏: –æ–±–∑–æ—Ä, link: https://habr.com/ru/companies/bothub/articles/842816/, snippet: –ê–≥–µ–Ω—Ç—ã LLM ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. –û–Ω–∏ —Å–ø–æ—Å–æ–±–Ω—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –∏ –¥—Ä—É–≥–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏, –∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π., title: –ê–≥–µ–Ω—Ç—ã –ù–∞ –û—Å–Ω–æ–≤–µ –ë–æ–ª—å—à–∏—Ö –Ø–∑—ã–∫–æ–≤—ã—Ö –ú–æ–¥–µ–ª–µ–π (Llm): –û–±–∑–æ—Ä, –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ò ..., link: https://nerdit.ru/llm-agents/, snippet: LLM (Large Language Model, –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å) ‚Äî —ç—Ç–æ —Ç–∏–ø –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–∏–ª–∏ —Ç–∏–ø –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ª—é–±—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (Natural Language Processing, NLP)., title: –ë–æ–ª—å—à–æ–π –æ–±–∑–æ—Ä –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π / –•–∞–±—Ä, link: https://habr.com/ru/companies/gaz-is/articles/884410/, snippet: –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –î–∞–Ω–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ä—ã–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –≤ –ø–µ—Ä–∏–æ–¥ 2022-2025 –≥–≥., —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ–µ‚Ä¶, title: –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö ..., link: https://ai.radensa.ru/archives/1696\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "<|assistant|> From the search results, I have gathered some useful information about studying large language models and agents. Here's a summary of key points that can help in creating an effective learning plan:\n",
            "\n",
            "1. Understanding the basics: Start by gaining knowledge on machine learning (ML) concepts such as neural networks, deep learning, training algorithms, optimization techniques, etc., to have a solid foundation for studying large language models.\n",
            "2. Research papers and surveys: Read research papers and survey articles about large language models (LLMs), intelligent agents using LLMs, and their applications in various domains like software engineering, human-agent interaction, and NLP tasks. Some key sources are available at the provided links: https://habr.com/ru/companies/bothub/, https://nerdit.ru/llm-agents/, https://habr.com/ru/companies/gaz-is/articles/884410/.\n",
            "3. Online courses and tutorials: Enroll in online courses on platforms like Coursera, edX, or fast.ai to learn more about large language models and related topics such as deep learning frameworks (TensorFlow, PyTorch), natural language processing (NLP) techniques, and reinforcement learning for intelligent agents.\n",
            "4. Implementing LLM projects: Work on open-source projects that involve using LLMs to build your own NLP models or agent systems. This can help you gain hands-on experience in applying these concepts to solve real-world problems.\n",
            "5. Participate in forums and communities: Engage with online forums like Reddit's Machine Learning subreddit, GitHub repositories related to LLM projects, and NLP research groups on LinkedIn or ResearchGate. Collaborating with others can provide valuable insights, guidance, and feedback.\n",
            "6. Stay updated with the latest developments: Follow relevant blogs, newsletters, and social media accounts of leading organizations working in the field of AI, such as Google's DeepMind, Microsoft's GitHub repository, or Facebook AI Research (FAIR). This will help you stay informed about new advancements.\n",
            "\n",
            "Final Answer: To create an effective learning plan for studying large language models and agents, follow these steps: 1) Understand the basics of\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '–ù–∞–ø–∏—à–∏ –ø–ª–∞–Ω –∏–∑—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∞–≥–µ–Ω—Ç–æ–≤, –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∫–∞—á–∫–∏ –≤ —ç—Ç–æ–π —Ç–µ–º–µ.',\n",
              " 'output': 'To create an effective learning plan for studying large language models and agents, follow these steps: 1) Understand the basics of'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the Price of a MacBook Pro?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": msg_eng\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "QTGSamSyyDFH",
        "outputId": "e76b80b7-723a-4870-e007-5c956d56c0d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to gather information about effective study methods and resources for learning about large language models and agents.\n",
            "Action: duckduck\n",
            "Action Input: \"best practices for studying large language models and agents\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: In 2016, the company revisited that article and talked about how the advent of machine learning and natural language processing could truly open up opportunities. Today, artificial intelligence ..., title: Best Practices For Consulting Firms Exploring Large Language Models, link: https://www.forbes.com/councils/forbesbusinesscouncil/2025/02/04/best-practices-for-consulting-firms-exploring-large-language-models/, snippet: Even though such metrics help evaluate LLMs, it is best to pair them with other methods, especially when the activities being handled are complex, critical and require human judgment. LLM Evaluation Benchmarks: LLM evaluation benchmarks help evaluate the large language model's performance by offering a standardized set of tasks., title: A Ready Guide to Large Language Model Evaluation: Metrics, Benchmarks ..., link: https://www.tredence.com/blog/llm-evaluation, snippet: Then, in the section \"hallenges and approaches of LLM agent-based modeling and simulation\", we will elaborate on the recent advances of large language model agent-based modeling and simulation ..., title: Large language models empowered agent-based modeling and ... - Nature, link: https://www.nature.com/articles/s41599-024-03611-3, snippet: An Overview of Large Language Model. At its core, a Large Language Model (LLM) is a type of machine learning model designed to understand, generate, and manipulate human language. These models are trained on massive amounts of text data using deep learning algorithms, enabling them to predict and generate language outputs based on the input they receive., title: Best Practices and Metrics for Evaluating Large Language Models (LLMs), link: https://www.frugaltesting.com/blog/best-practices-and-metrics-for-evaluating-large-language-models-llms\u001b[0m\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: To study large language models and agents effectively, you should follow these best practices and metrics for evaluation as suggested by various sources. Begin by understanding the fundamentals of machine learning, natural language processing (NLP), and deep learning algorithms through reputable online courses or textbooks. Utilize resources such as Forbes Business Council's article on \"Best Practices For Consulting Firms Exploring Large Language Models\" for practical insights into the field. Additionally, explore Nature's overview of large language models to grasp their core concepts and applications.\n",
            "\n",
            "For evaluating LLMs, consider reading FrugalTesting's \"Best Practices and Metrics for Evaluating Large Language Models (LLMs)\" article to gain a comprehensive understanding of evaluation benchmarks and metrics used in the industry. Furthermore, Tredence's guide on \"hallenges and approaches of LLM agent-based modeling and simulation\" provides valuable information on recent advances in this area.\n",
            "\n",
            "To deepen your knowledge and stay updated with the latest research, subscribe to relevant journals such as Nature Communications or join online forums and communities where professionals discuss large language models and agents. Finally, practice hands-on experiments by working with available datasets and tools like GPT-3 API offered by Microsoft or similar platforms.\n",
            "\n",
            "By combining theoretical knowledge, practical applications, industry insights, evaluation metrics, and continuous learning, you can maximize improvement in studying large language models and agents.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Write a plan for studying large language models and agents, for maximum improvement in this topic.',\n",
              " 'output': 'To study large language models and agents effectively, you should follow these best practices and metrics for evaluation as suggested by various sources. Begin by understanding the fundamentals of machine learning, natural language processing (NLP), and deep learning algorithms through reputable online courses or textbooks. Utilize resources such as Forbes Business Council\\'s article on \"Best Practices For Consulting Firms Exploring Large Language Models\" for practical insights into the field. Additionally, explore Nature\\'s overview of large language models to grasp their core concepts and applications.\\n\\nFor evaluating LLMs, consider reading FrugalTesting\\'s \"Best Practices and Metrics for Evaluating Large Language Models (LLMs)\" article to gain a comprehensive understanding of evaluation benchmarks and metrics used in the industry. Furthermore, Tredence\\'s guide on \"hallenges and approaches of LLM agent-based modeling and simulation\" provides valuable information on recent advances in this area.\\n\\nTo deepen your knowledge and stay updated with the latest research, subscribe to relevant journals such as Nature Communications or join online forums and communities where professionals discuss large language models and agents. Finally, practice hands-on experiments by working with available datasets and tools like GPT-3 API offered by Microsoft or similar platforms.\\n\\nBy combining theoretical knowledge, practical applications, industry insights, evaluation metrics, and continuous learning, you can maximize improvement in studying large language models and agents.'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1. Understanding the basics: Start by gaining knowledge on machine learning (ML) concepts such as neural networks, deep learning, training algorithms,\n",
        "optimization techniques, etc., to have a solid foundation for studying large language models.\n",
        "2. Research papers and surveys: Read research papers and survey articles about large language models (LLMs), intelligent agents using LLMs, and their applications\n",
        "in various domains like software engineering, human-agent interaction, and NLP tasks. Some key sources are available at the provided links: https://habr.com/ru/companies/bothub/, https://nerdit.ru/llm-agents/, https://habr.com/ru/companies/gaz-is/articles/884410/.\n",
        "3. Online courses and tutorials: Enroll in online courses on platforms like Coursera, edX, or fast.ai to learn more about large language models and related topics such as deep learning frameworks (TensorFlow, PyTorch), natural language processing (NLP) techniques, and reinforcement learning for intelligent agents.\n",
        "4. Implementing LLM projects: Work on open-source projects that involve using LLMs to build your own NLP models or agent systems. This can help you gain hands-on experience in applying these concepts to solve real-world problems.\n",
        "5. Participate in forums and communities: Engage with online forums like Reddit's Machine Learning subreddit, GitHub repositories related to LLM projects, a\n",
        "nd NLP research groups on LinkedIn or ResearchGate. Collaborating with others can provide valuable insights, guidance, and feedback.\n",
        "6. Stay updated with the latest developments: Follow relevant blogs, newsletters, and social media accounts of leading organizations working in the field of AI,\n",
        "such as Google's DeepMind, Microsoft's GitHub repository, or Facebook AI Research (FAIR). This will help you stay informed about new advancements.\n"
      ],
      "metadata": {
        "id": "ODQhhMxMyMN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}