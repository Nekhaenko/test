{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "sudo apt-get install zstd\n",
        "\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "ollama serve &\n",
        "\n",
        "ollama pull functiongemma phi3\n",
        "ollama run functiongemma\n",
        "ollama run gemma3:1b"
      ],
      "metadata": {
        "id": "UIEVYVVIrz4Q"
      },
      "id": "UIEVYVVIrz4Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U langchain-ollama"
      ],
      "metadata": {
        "id": "R-3O98E6ucsN"
      },
      "id": "R-3O98E6ucsN",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "model = ChatOllama(\n",
        "    model=\"gemma3:1b\",\n",
        "    validate_model_on_init=True,\n",
        "    temperature=0.8,\n",
        "    num_predict=256,\n",
        ")"
      ],
      "metadata": {
        "id": "MCGbYfXhruCx"
      },
      "id": "MCGbYfXhruCx",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = None, cache: BaseCache | bool | None = None,\n",
        "verbose: bool = _get_verbosity,\n",
        "callbacks: Callbacks = None, tags: list[str] | None = None,\n",
        "metadata: dict[str, Any] | None = None,\n",
        "custom_get_token_ids: ((str) -> list[int]) | None = None,\n",
        "rate_limiter: BaseRateLimiter | None = None,\n",
        "disable_streaming: bool | Literal['tool_calling'] = False,\n",
        "output_version: str | None = from_env(\"LC_OUTPUT_VERSION\", default=None),\n",
        "profile: ModelProfile | None = None, model: str,\n",
        "reasoning: bool | str | None = None, validate_model_on_init: bool = False,\n",
        "mirostat: int | None = None, mirostat_eta: float | None = None,\n",
        "mirostat_tau: float | None = None, num_ctx: int | None = None,\n",
        "num_gpu: int | None = None, num_thread: int | None = None, num_predict: int | None = None,\n",
        "repeat_last_n: int | None = None, repeat_penalty: float | None = None,\n",
        "temperature: float | None = None, seed: int | None = None, stop: list[str] | None = None,\n",
        "tfs_z: float | None = None, top_k: int | None = None, top_p: float | None = None,\n",
        "format: JsonSchemaValue | Literal['', 'json'] | None = None, keep_alive: int | str | None = None,\n",
        "base_url: str | None = None, client_kwargs: dict | None = {},\n",
        "async_client_kwargs: dict | None = {}, sync_client_kwargs: dict | None = {}) -> ChatOllama\n",
        "Ollama chat model integration.\n",
        "\n",
        "???+ note \"Setup\"\n",
        "\n",
        "    Install langchain-ollama and download any models you want to use from ollama.\n",
        "\n",
        "    ollama pull gpt-oss:20b\n",
        "    pip install -U langchain-ollama\n",
        "Key init args — completion params:\n",
        "    model: str\n",
        "        Name of Ollama model to use.\n",
        "    reasoning: bool | None\n",
        "        Controls the reasoning/thinking mode for\n",
        "        supported models.\n",
        "\n",
        "- `True`: Enables reasoning mode. The model's reasoning process will be\n",
        "captured and returned separately in the additional_kwargs of the\n",
        "response message, under reasoning_content. The main response\n",
        "content will not include the reasoning tags.\n",
        "    - False: Disables reasoning mode. The model will not perform any reasoning,\n",
        "and the response will not include any reasoning content.\n",
        "    - None (Default): The model will use its default reasoning behavior. Note\n",
        "however, if the model's default behavior *is* to perform reasoning, think tags\n",
        "(<think> and </think>) will be present within the main response content\n",
        "unless you set reasoning to True.\n",
        "temperature: float\n",
        "Sampling temperature. Ranges from 0.0 to 1.0.\n",
        "num_predict: int | None\n",
        "Max number of tokens to generate.\n",
        "\n",
        "See full list of supported init args and their descriptions in the params section.\n",
        "\n",
        "Instantiate:\n",
        "\n",
        "    from langchain_ollama import ChatOllama\n",
        "\n",
        "    model = ChatOllama(\n",
        "        model=\"gpt-oss:20b\",\n",
        "        validate_model_on_init=True,\n",
        "        temperature=0.8,\n",
        "        num_predict=256,\n",
        "        # other params ...\n",
        "    )\n",
        "Invoke:\n",
        "\n",
        "    messages = [\n",
        "        (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
        "        (\"human\", \"I love programming.\"),\n",
        "    ]\n",
        "    model.invoke(messages)\n",
        "    AIMessage(content='J'adore le programmation. (Note: \"programming\" can also refer to the act of writing code, so if you meant that, I could translate it as \"J'adore programmer\". But since you didn\\'t specify, I assumed you were talking about the activity itself, which is what \"le programmation\" usually refers to.)', response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:37:50.182604Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3576619666, 'load_duration': 788524916, 'prompt_eval_count': 32, 'prompt_eval_duration': 128125000, 'eval_count': 71, 'eval_duration': 2656556000}, id='run-ba48f958-6402-41a5-b461-5e250a4ebd36-0')\n",
        "Stream:\n",
        "\n",
        "    for chunk in model.stream(\"Return the words Hello World!\"):\n",
        "        print(chunk.text, end=\"\")\n",
        "    content='Hello' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
        "    content=' World' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
        "    content='!' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
        "    content='' response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:39:42.274449Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 411875125, 'load_duration': 1898166, 'prompt_eval_count': 14, 'prompt_eval_duration': 297320000, 'eval_count': 4, 'eval_duration': 111099000} id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
        "    stream = model.stream(messages)\n",
        "    full = next(stream)\n",
        "    for chunk in stream:\n",
        "        full += chunk\n",
        "    full\n",
        "    AIMessageChunk(\n",
        "        content='Je adore le programmation.(Note: \"programmation\" is the formal way to say \"programming\" in French, but informally, people might use the phrase \"le développement logiciel\" or simply \"le code\")',\n",
        "        response_metadata={\n",
        "            \"model\": \"llama3\",\n",
        "            \"created_at\": \"2024-07-04T03:38:54.933154Z\",\n",
        "            \"message\": {\"role\": \"assistant\", \"content\": \"\"},\n",
        "            \"done_reason\": \"stop\",\n",
        "            \"done\": True,\n",
        "            \"total_duration\": 1977300042,\n",
        "            \"load_duration\": 1345709,\n",
        "            \"prompt_eval_duration\": 159343000,\n",
        "            \"eval_count\": 47,\n",
        "            \"eval_duration\": 1815123000,\n",
        "        },\n",
        "        id=\"run-3c81a3ed-3e79-4dd3-a796-04064d804890\",\n",
        "    )\n",
        "Async:\n",
        "\n",
        "    await model.ainvoke(\"Hello how are you!\")\n",
        "    AIMessage(\n",
        "        content=\"Hi there! I'm just an AI, so I don't have feelings or emotions like humans do. But I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\",\n",
        "        response_metadata={\n",
        "            \"model\": \"llama3\",\n",
        "            \"created_at\": \"2024-07-04T03:52:08.165478Z\",\n",
        "            \"message\": {\"role\": \"assistant\", \"content\": \"\"},\n",
        "            \"done_reason\": \"stop\",\n",
        "            \"done\": True,\n",
        "            \"total_duration\": 2138492875,\n",
        "            \"load_duration\": 1364000,\n",
        "            \"prompt_eval_count\": 10,\n",
        "            \"prompt_eval_duration\": 297081000,\n",
        "            \"eval_count\": 47,\n",
        "            \"eval_duration\": 1838524000,\n",
        "        },\n",
        "        id=\"run-29c510ae-49a4-4cdd-8f23-b972bfab1c49-0\",\n",
        "    )\n",
        "    async for chunk in model.astream(\"Say hello world!\"):\n",
        "        print(chunk.content)\n",
        "    HEL\n",
        "    LO\n",
        "    WORLD\n",
        "    !\n",
        "    messages = [(\"human\", \"Say hello world!\"), (\"human\", \"Say goodbye world!\")]\n",
        "    await model.abatch(messages)\n",
        "    [\n",
        "        AIMessage(\n",
        "            content=\"HELLO, WORLD!\",\n",
        "            response_metadata={\n",
        "                \"model\": \"llama3\",\n",
        "                \"created_at\": \"2024-07-04T03:55:07.315396Z\",\n",
        "                \"message\": {\"role\": \"assistant\", \"content\": \"\"},\n",
        "                \"done_reason\": \"stop\",\n",
        "                \"done\": True,\n",
        "                \"total_duration\": 1696745458,\n",
        "                \"load_duration\": 1505000,\n",
        "                \"prompt_eval_count\": 8,\n",
        "                \"prompt_eval_duration\": 111627000,\n",
        "                \"eval_count\": 6,\n",
        "                \"eval_duration\": 185181000,\n",
        "            },\n",
        "            id=\"run-da6c7562-e25a-4a44-987a-2c83cd8c2686-0\",\n",
        "        ),\n",
        "        AIMessage(\n",
        "            content=\"It's been a blast chatting with you! Say goodbye to the world for me, and don't forget to come back and visit us again soon!\",\n",
        "            response_metadata={\n",
        "                \"model\": \"llama3\",\n",
        "                \"created_at\": \"2024-07-04T03:55:07.018076Z\",\n",
        "                \"message\": {\"role\": \"assistant\", \"content\": \"\"},\n",
        "                \"done_reason\": \"stop\",\n",
        "                \"done\": True,\n",
        "                \"total_duration\": 1399391083,\n",
        "                \"load_duration\": 1187417,\n",
        "                \"prompt_eval_count\": 20,\n",
        "                \"prompt_eval_duration\": 230349000,\n",
        "                \"eval_count\": 31,\n",
        "                \"eval_duration\": 1166047000,\n",
        "            },\n",
        "            id=\"run-96cad530-6f3e-4cf9-86b4-e0f8abba4cdb-0\",\n",
        "        ),\n",
        "    ]\n",
        "JSON mode:\n",
        "\n",
        "    json_model = ChatOllama(format=\"json\")\n",
        "    json_model.invoke(\n",
        "        \"Return a query for the weather in a random location and time of day with two keys: location and time_of_day. \"\n",
        "        \"Respond using JSON only.\"\n",
        "    ).content\n",
        "    '{\"location\": \"Pune, India\", \"time_of_day\": \"morning\"}'\n",
        "Tool Calling:\n",
        "\n",
        "    from langchain_ollama import ChatOllama\n",
        "    from pydantic import BaseModel, Field\n",
        "\n",
        "    class Multiply(BaseModel):\n",
        "        a: int = Field(..., description=\"First integer\")\n",
        "        b: int = Field(..., description=\"Second integer\")\n",
        "\n",
        "    ans = await chat.invoke(\"What is 45*67\")\n",
        "    ans.tool_calls\n",
        "    [\n",
        "        {\n",
        "            \"name\": \"Multiply\",\n",
        "            \"args\": {\"a\": 45, \"b\": 67},\n",
        "            \"id\": \"420c3f3b-df10-4188-945f-eb3abdb40622\",\n",
        "            \"type\": \"tool_call\",\n",
        "        }\n",
        "    ]\n",
        "Thinking / Reasoning:\n",
        "    You can enable reasoning mode for models that support it by setting\n",
        "the reasoning parameter to True in either the constructor or\n",
        "the invoke/stream methods. This will enable the model to think\n",
        "through the problem and return the reasoning process separately in the\n",
        "additional_kwargs of the response message, under reasoning_content.\n",
        "\n",
        "    If reasoning is set to None, the model will use its default reasoning\n",
        "behavior, and any reasoning content will *not* be captured under the\n",
        "reasoning_content key, but will be present within the main response content\n",
        "as think tags (<think> and </think>).\n",
        "\n",
        "    !!! note\n",
        "        This feature is only available for models that support reasoning.\n",
        "\n",
        "    from langchain_ollama import ChatOllama\n",
        "\n",
        "    model = ChatOllama(\n",
        "        model=\"deepseek-r1:8b\",\n",
        "        validate_model_on_init=True,\n",
        "        reasoning=True,\n",
        "    )\n",
        "\n",
        "    model.invoke(\"how many r in the word strawberry?\")\n",
        "\n",
        "    # or, on an invocation basis:\n",
        "\n",
        "    model.invoke(\"how many r in the word strawberry?\", reasoning=True)\n",
        "    # or model.stream(\"how many r in the word strawberry?\", reasoning=True)\n",
        "\n",
        "    # If not provided, the invocation will default to the ChatOllama reasoning\n",
        "    # param provided (None by default).\n",
        "    AIMessage(content='The word \"strawberry\" contains **three \\'r\\' letters**. Here\\'s a breakdown for clarity:\\n\\n- The spelling of \"strawberry\" has two parts ... be 3.\\n\\nTo be thorough, let\\'s confirm with an online source or common knowledge.\\n\\nI can recall that \"strawberry\" has: s-t-r-a-w-b-e-r-r-y — yes, three r\\'s.\\n\\nPerhaps it\\'s misspelled by some, but standard is correct.\\n\\nSo I think the response should be 3.\\n'}, response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-07-08T19:33:55.891269Z', 'done': True, 'done_reason': 'stop', 'total_duration': 98232561292, 'load_duration': 28036792, 'prompt_eval_count': 10, 'prompt_eval_duration': 40171834, 'eval_count': 3615, 'eval_duration': 98163832416, 'model_name': 'deepseek-r1:8b'}, id='run--18f8269f-6a35-4a7c-826d-b89d52c753b3-0', usage_metadata={'input_tokens': 10, 'output_tokens': 3615, 'total_tokens': 3625})"
      ],
      "metadata": {
        "id": "YmC9FwNjwhSJ"
      },
      "id": "YmC9FwNjwhSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "\n",
        "system_msg = SystemMessage(\"Ты оператор службы поддержки.\")\n",
        "human_msg = HumanMessage(\"Как восстановить пароль?\")\n",
        "\n",
        "# Use with chat models\n",
        "messages = [system_msg, human_msg]\n",
        "response = model.invoke(messages)  # Returns AIMessage"
      ],
      "metadata": {
        "id": "3IdJDe_mzASA"
      },
      "id": "3IdJDe_mzASA",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvDAiRpH2Bcc",
        "outputId": "8edc700a-ef7d-44ff-8378-579cd7f9e63b"
      },
      "id": "kvDAiRpH2Bcc",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Здравствуйте! Я ваш оператор службы поддержки. Конечно, я помогу вам восстановить ваш пароль. \\n\\nЧтобы я мог вам помочь, мне потребуется немного информации. Пожалуйста, ответьте на следующие вопросы:\\n\\n1.  **Какой у вас учетный учет?** (Например: Google, Facebook, Microsoft, и т.д.)\\n2.  **Как вы зарегистрировались на этот сервис?** (Например: через email, через приложение, через веб-сайт?)\\n3.  **Вы помните свой email или номер телефона, который вы использовали при регистрации?** (Это может быть полезно, если у вас есть возможность его вспомнить.)\\n4.  **Если вы не помните email или номер телефона, укажите, если вы использовали какие-либо другие способы?** (Например, данные были указаны при регистрации, или вы используете другой метод, который вы помните?)\\n\\nПосле того, как я узнаю эту информацию, я смогу предложить вам несколько вариантов восстановления.\\n\\n**В качестве общего примера, вот некоторые вещи, которые обычно делают:**\\n\\n*   **Восстановление через email:** Если вы зарегистрировались через email, я могу попросить вас ввести адрес электронной почты', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2026-01-16T08:10:50.294768007Z', 'done': True, 'done_reason': 'length', 'total_duration': 43895984432, 'load_duration': 642974103, 'prompt_eval_count': 26, 'prompt_eval_duration': 251658025, 'eval_count': 256, 'eval_duration': 42159975018, 'logprobs': None, 'model_name': 'gemma3:1b', 'model_provider': 'ollama'}, id='lc_run--019bc5da-c0ba-7102-ba66-a04749f10dbc-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 26, 'output_tokens': 256, 'total_tokens': 282})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_msg = HumanMessage(\n",
        "    content=\"Как восстановить пароль?\",\n",
        "    name=\"alice\",  # Optional: identify different users\n",
        "    id=\"msg_123\",  # Optional: unique identifier for tracing\n",
        ")\n",
        "messages = [system_msg, human_msg]\n",
        "response = model.invoke(messages)"
      ],
      "metadata": {
        "id": "skCKDu_C2eJK"
      },
      "id": "skCKDu_C2eJK",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [(\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
        "            (\"human\", \"I love programming.\")]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmTGWNrNs29l",
        "outputId": "2a51dbc5-550d-4a74-c063-049807631d71"
      },
      "id": "RmTGWNrNs29l",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Here are a few options for translating “I love programming” into French, with slight nuances:\\n\\n*   **J\\'aime programmer.** (This is the most common and natural way to say it.)\\n*   **J\\'adore programmer.** (This expresses a stronger feeling of love/passion - “I adore programming.”)\\n*   **Je suis passionné(e) par la programmation.** (This is a more formal way of saying \"I am passionate about programming.\") - Use \"passionné\" if you are male and “passionnée” if you are female.\\n\\n**I recommend using: J\\'aime programmer.** \\n\\nWould you like me to translate something else?', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2026-01-16T07:55:52.472571309Z', 'done': True, 'done_reason': 'stop', 'total_duration': 44582740828, 'load_duration': 1008529703, 'prompt_eval_count': 31, 'prompt_eval_duration': 1970591259, 'eval_count': 141, 'eval_duration': 23164607978, 'logprobs': None, 'model_name': 'gemma3:1b', 'model_provider': 'ollama'}, id='lc_run--019bc5cd-0aeb-7e72-9442-a23e276224fb-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 31, 'output_tokens': 141, 'total_tokens': 172})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in model.stream(\"Return the words Hello World!\"):\n",
        "    print(chunk.text, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZkbaYi8usC0",
        "outputId": "f5ca1a84-6761-4ba8-9a7c-16f0d4083111"
      },
      "id": "sZkbaYi8usC0",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am sorry, but I cannot assist with this request. My current capabilities are limited to assisting with tasks related to text generation and conversation. I cannot generate or retrieve word definitions."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather for a given city.\"\"\"\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        "    system_prompt=\"You are a helpful assistant\",\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "Vnnv7V-Knj6n",
        "outputId": "f81983b5-296d-4fa4-a927-577af558287c"
      },
      "id": "Vnnv7V-Knj6n",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResponseError",
          "evalue": "registry.ollama.ai/library/gemma3:1b does not support tools (status code: 400)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-126405283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Run the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m agent.invoke(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"what is the weather in sf\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2644\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/factory.py\u001b[0m in \u001b[0;36mmodel_node\u001b[0;34m(state, runtime)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrap_model_call_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;31m# No handlers - execute directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute_model_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0;31m# Call composed handler with base handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/factory.py\u001b[0m in \u001b[0;36m_execute_model_sync\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5555\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5556\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5557\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5558\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5559\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m             cast(\n\u001b[1;32m    401\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    403\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1120\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 results.append(\n\u001b[0;32m--> 931\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    932\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1226\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_ollama/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     ) -> ChatResult:\n\u001b[0;32m-> 1030\u001b[0;31m         final_chunk = self._chat_stream_with_aggregation(\n\u001b[0m\u001b[1;32m   1031\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_ollama/chat_models.py\u001b[0m in \u001b[0;36m_chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m     ) -> ChatGenerationChunk:\n\u001b[1;32m    964\u001b[0m         \u001b[0mfinal_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterate_over_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfinal_chunk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0mfinal_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_ollama/chat_models.py\u001b[0m in \u001b[0;36m_iterate_over_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m     ) -> Iterator[ChatGenerationChunk]:\n\u001b[1;32m   1053\u001b[0m         \u001b[0mreasoning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reasoning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreasoning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstream_resp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_resp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 content = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_ollama/chat_models.py\u001b[0m in \u001b[0;36m_create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchat_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mchat_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mchat_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ollama/_client.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResponseError\u001b[0m: registry.ollama.ai/library/gemma3:1b does not support tools (status code: 400)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import SummarizationMiddleware\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[],\n",
        "    middleware=[\n",
        "        SummarizationMiddleware(\n",
        "            model=model,\n",
        "            trigger=(\"tokens\", 4000),\n",
        "            keep=(\"messages\", 20)\n",
        "        )\n",
        "    ],\n",
        "    checkpointer=checkpointer,\n",
        ")\n",
        "\n",
        "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
        "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
        "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
        "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
        "\n",
        "final_response[\"messages\"][-1].pretty_print()\n",
        "\"\"\"\n",
        "================================== Ai Message ==================================\n",
        "\n",
        "Your name is Bob!\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "z6uxvGlu3FUO",
        "outputId": "5eeb607c-1c15-499c-e635-d1f8b66ff73d"
      },
      "id": "z6uxvGlu3FUO",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I am a large language model, and I don't have access to personal information like names. 😊 \n",
            "\n",
            "It’s a nice mystery, isn’t it? 😉\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================== Ai Message ==================================\\n\\nYour name is Bob!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.educative.io/blog/ollama-guide"
      ],
      "metadata": {
        "id": "W3yplXvhtvX6"
      },
      "id": "W3yplXvhtvX6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d1c538",
      "metadata": {
        "id": "c2d1c538"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import PyPDF2\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st.set_page_config(page_title=\"Ollama RAG Chatbot\", page_icon=\"🤖\", layout=\"wide\")"
      ],
      "metadata": {
        "id": "e5aFR07sP7bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7722d12-8159-4155-ca15-298b7c5904cd"
      },
      "id": "e5aFR07sP7bg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b86621",
      "metadata": {
        "id": "67b86621"
      },
      "outputs": [],
      "source": [
        "with st.sidebar:\n",
        "    st.title(\"PDF Upload\")\n",
        "    uploaded_file = st.file_uploader(\"Upload a PDF file\", type=\"pdf\")\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        st.success(\"PDF uploaded successfully!\")\n",
        "\n",
        "        # PDF preview\n",
        "        st.subheader(\"PDF Preview\")\n",
        "        pdf_document = fitz.open(stream=uploaded_file.read(), filetype=\"pdf\")\n",
        "        num_pages = len(pdf_document)\n",
        "        page_num = st.number_input(\"Page\", min_value=1, max_value=num_pages, value=1)\n",
        "\n",
        "        page = pdf_document.load_page(page_num - 1)\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "        st.image(img, caption=f\"Page {page_num}\", use_column_width=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe94773",
      "metadata": {
        "id": "3fe94773"
      },
      "outputs": [],
      "source": [
        "st.title(\"Ollama RAG Chatbot with Latest Llama Model\")\n",
        "\n",
        "if \"chain\" not in st.session_state:\n",
        "    st.session_state.chain = None\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2c9344",
      "metadata": {
        "id": "eb2c9344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "c181c894-6b4a-41e7-cd16-9050acff219e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      commit  \\\n",
              "0   44e41c12ab25e36c202f58e068ced262eadc8d16   \n",
              "1   e66a40038e3c84fb1a68da67ad71caf75c64a027   \n",
              "2   c6a930897e9f9e9878db031cc7fb6ea79d721a74   \n",
              "\n",
              "                                              author  \\\n",
              "0  Lakshmi Narayanan Sreethar<lakshmi@timescale.com>   \n",
              "1                    Bharathy<satish.8483@gmail.com>   \n",
              "2                  Jan Nidzwetzki<jan@timescale.com>   \n",
              "\n",
              "                             date  \\\n",
              "0   Tue Sep 5 21:03:21 2023 +0530   \n",
              "1   Sat Sep 2 09:24:31 2023 +0530   \n",
              "2  Tue Aug 29 21:13:51 2023 +0200   \n",
              "\n",
              "                                   change summary  \\\n",
              "0            Fix segfault in set_integer_now_func   \n",
              "1  Fix server crash on UPDATE of compressed chunk   \n",
              "2            Use Debian Bookworm for 32-bit tests   \n",
              "\n",
              "                                      change details  \n",
              "0  When an invalid function oid is passed to set_...  \n",
              "1  UPDATE query with system attributes in WHERE c...  \n",
              "2  So far, we have used Debian Buster (10) for ou...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f09704e7-7271-42b9-b7c9-78813e66fa95\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>commit</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>change summary</th>\n",
              "      <th>change details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>44e41c12ab25e36c202f58e068ced262eadc8d16</td>\n",
              "      <td>Lakshmi Narayanan Sreethar&lt;lakshmi@timescale.com&gt;</td>\n",
              "      <td>Tue Sep 5 21:03:21 2023 +0530</td>\n",
              "      <td>Fix segfault in set_integer_now_func</td>\n",
              "      <td>When an invalid function oid is passed to set_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e66a40038e3c84fb1a68da67ad71caf75c64a027</td>\n",
              "      <td>Bharathy&lt;satish.8483@gmail.com&gt;</td>\n",
              "      <td>Sat Sep 2 09:24:31 2023 +0530</td>\n",
              "      <td>Fix server crash on UPDATE of compressed chunk</td>\n",
              "      <td>UPDATE query with system attributes in WHERE c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c6a930897e9f9e9878db031cc7fb6ea79d721a74</td>\n",
              "      <td>Jan Nidzwetzki&lt;jan@timescale.com&gt;</td>\n",
              "      <td>Tue Aug 29 21:13:51 2023 +0200</td>\n",
              "      <td>Use Debian Bookworm for 32-bit tests</td>\n",
              "      <td>So far, we have used Debian Buster (10) for ou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f09704e7-7271-42b9-b7c9-78813e66fa95')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f09704e7-7271-42b9-b7c9-78813e66fa95 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f09704e7-7271-42b9-b7c9-78813e66fa95');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6bff1ea6-ba05-4b34-8787-99a611b3c2f3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6bff1ea6-ba05-4b34-8787-99a611b3c2f3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6bff1ea6-ba05-4b34-8787-99a611b3c2f3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"commit\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \" 7600896a66e55fd933e81433f1aceecc7ddfc1c6\",\n          \" 1ef515eb7a5ca0c00f78c552ba9f7c20b67423a5\",\n          \" 65b0dda97b11224e79f3aa7580b714337718308c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 41,\n        \"samples\": [\n          \"Pallavi Sontakke<pallavi@timescale.com>\",\n          \"Markus Engel<engel@sero-systems.de>\",\n          \"Erik Nordstr\\u00f6m<erik@timescale.com>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 995,\n        \"samples\": [\n          \"Fri Jan 28 13:30:32 2022 +0100\",\n          \"Sat Oct 1 21:19:28 2022 +0200\",\n          \"Tue Sep 13 23:05:30 2022 +0200\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"change summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 991,\n        \"samples\": [\n          \"Look up compressed column metadata only at planning time\",\n          \"Post-release fixes for 2.9.2\",\n          \"Speed up the dist_copy tests\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"change details\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 997,\n        \"samples\": [\n          \"We don't want to support BitmapScans below DecompressChunk as this adds additional complexity to support and there is little benefit in doing so. This fixes a bug that can happen when we have a parameterized BitmapScan that is parameterized on a compressed column and will lead to an execution failure with an error regarding incorrect attribute types in the expression.\",\n          \"In #4199 existing calls of `ereport` were replaced with calls of `write_stderr` to eliminate the use of signal-unsafe calls, in particular calls to `malloc`. Unfortunately, `write_stderr` contains a call to `vfprintf`, which allocates memory as well, occationally causing servers that are shutting down to become unresponsive.  Since the existing signal handlers just called `die` after printing out a useful message, this commit fixes this by using `die` as a signal handler.  Fixes #4200 \",\n          \"Commit 8afdddc2da added the first step for deprecating the old format of Continuous Aggregate but just for PostgreSQL 15 and later versions.  During the extension update we emit a message about the deprecation but this has being emited even if the user is using PostgreSQL versions before 15.  Fixed it by emiting the WARNING just when PostgreSQL version is greater or equal to 15.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def process_pdf(file):\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    pdf_text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        pdf_text += page.extract_text()\n",
        "    return pdf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef55d54",
      "metadata": {
        "id": "2ef55d54"
      },
      "outputs": [],
      "source": [
        "if uploaded_file is not None:\n",
        "    if st.session_state.chain is None:\n",
        "        with st.spinner(\"Processing PDF...\"):\n",
        "            pdf_text = process_pdf(uploaded_file)\n",
        "\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "            texts = text_splitter.split_text(pdf_text)\n",
        "\n",
        "            metadatas = [{\"source\": f\"chunk_{i}\"} for i in range(len(texts))]\n",
        "\n",
        "            embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "            docsearch = Chroma.from_texts(texts, embeddings, metadatas=metadatas)\n",
        "\n",
        "            message_history = ChatMessageHistory()\n",
        "            memory = ConversationBufferMemory(\n",
        "                memory_key=\"chat_history\",\n",
        "                output_key=\"answer\",\n",
        "                chat_memory=message_history,\n",
        "                return_messages=True,\n",
        "            )\n",
        "\n",
        "            st.session_state.chain = ConversationalRetrievalChain.from_llm(\n",
        "                ChatOllama(model=\"llama3.1\", temperature=0.7),\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
        "                memory=memory,\n",
        "                return_source_documents=True,\n",
        "            )\n",
        "\n",
        "            st.success(\"PDF processed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d622744",
      "metadata": {
        "id": "2d622744"
      },
      "outputs": [],
      "source": [
        "st.subheader(\"Chat with your PDF\")\n",
        "user_input = st.text_input(\"Ask a question about the document:\")\n",
        "\n",
        "if user_input:\n",
        "    if st.session_state.chain is None:\n",
        "        st.warning(\"Please upload a PDF file first.\")\n",
        "    else:\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = st.session_state.chain.invoke({\"question\": user_input})\n",
        "            answer = response[\"answer\"]\n",
        "            source_documents = response[\"source_documents\"]\n",
        "\n",
        "            st.session_state.chat_history.append(HumanMessage(content=user_input))\n",
        "            st.session_state.chat_history.append(AIMessage(content=answer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626ad668",
      "metadata": {
        "id": "626ad668"
      },
      "outputs": [],
      "source": [
        "chat_container = st.container()\n",
        "with chat_container:\n",
        "    for message in reversed(st.session_state.chat_history):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            st.markdown(f'👤 {message.content}')\n",
        "        elif isinstance(message, AIMessage):\n",
        "            st.markdown(f'🤖 {message.content}')\n",
        "\n",
        "        if isinstance(message, AIMessage):\n",
        "            with st.expander(\"View Sources\"):\n",
        "                for idx, doc in enumerate(source_documents):\n",
        "                    st.write(f\"Source {idx + 1}:\", doc.page_content[:150] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58450993",
      "metadata": {
        "id": "58450993"
      },
      "outputs": [],
      "source": [
        "streamlit run rag_chatbot.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-5ATUeZ1ECE"
      },
      "id": "V-5ATUeZ1ECE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}